---
layout: post
title: Log
date: 2019-01-16 15:21 +0800
---

Log is core of distributed and real-time application architectures, in various form: write-ahead log, commit log or transition log.

# What is Log

A log is an append-only sequence of records totally-ordered by time, records what happened and when.

## Log in database

Log is used for implements replicating data between databases for implementation of ACID. And the log is used for subscription mechanism for data subscribers.

## Log in Distribute System

*If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state.*
-- State Machine Replication Principle

Log is used to keep distributed system consensus.

## Changelog 101: Tables and Events are Dual

Events are dynamic table, and table is a event snapshot.

Log is used for:

* Data integration
* Real-time data processing
* Distribute system design

# Data Integration

*Data integration is making all the data an organization has available in all its services and systems.* Or more generally *ETL*.

Reliable complete data flow is basic of the data processing system and for the .

# Data Integration: Two complications

## The event data firehose

The first trend is the rise of event data. Event data records things that happen rather than things that are. This type of data is magnitude larger than traditional database.

## The explosion of specialized data systems

There a lot more database system then ever: OLAP, search, simple online storage, batch processing, graph analysis and so on.

# Log-structured data flow

Each logical data source can be modeled as its own log.

A data source could be an application that logs out events, or a database table that accepts modifications.

Each subscribing system reads from this log as quick as it can, applies each new record to its own store, and advances its position in to log.

After suffers from data fetch and bad data quality, Linkedin build data pipelines, and data pipelines are:

1. The pipelines a extremely valuable.
2. Reliable data loads requires deep support from the data pipeline. If the pipelines covers all the data sources, we can get a reliable data lake of HDFS and Hive.
3. Data coverage is still low.

Data pipeline connect each data resource and destination is massive, instead we need a central pipeline to manage all the data.

